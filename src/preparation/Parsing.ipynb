{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "hHAjSAJ_ziPT"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ноут\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "EBQdBxnoz3NN"
   },
   "outputs": [],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "dr = webdriver.Chrome(options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "x_D3BnfqLWc1"
   },
   "outputs": [],
   "source": [
    "url = 'https://synonyms.su'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "2gyN0OHA0hA5"
   },
   "outputs": [],
   "source": [
    "dr.get('https://synonyms.su')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ea_cJjOi165M",
    "outputId": "a65c06b1-a748-4389-a414-b6dfa244b086"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<selenium.webdriver.remote.webelement.WebElement (session=\"6015fbedac249b5b33ccc19cb2eb164b\", element=\"f.3CC5B9F91D8D78A1B195316696938B8D.d.9D76487C4E9BBB0C1669FF77C6D1A991.e.36\")>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dr.find_elements(by=By.CLASS_NAME, value='letter-list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "IH5WIPyd4xpj"
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(dr.page_source, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "2OX1eL1gKUF7"
   },
   "outputs": [],
   "source": [
    "letter_refs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "E6AznrrX4_nT"
   },
   "outputs": [],
   "source": [
    "for alpha in soup.find('div', class_='letter-list').find_all('a'):\n",
    "    letter_refs.append(alpha.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dictionaries = ['https://synonymonline.ru/', 'https://synonyms.su/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "letter_pages_count = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ref in letter_refs:\n",
    "    dr.get(url+ref)\n",
    "    soup1 = BeautifulSoup(dr.page_source, 'lxml')\n",
    "    if soup1.find('nav', class_='pagination') is not None:\n",
    "        letter_pages_count[ref] = int(soup1.find('nav', class_='pagination').find_all('a')[-1].contents[0])\n",
    "    else:\n",
    "        letter_pages_count[ref] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/a': 14,\n",
       " '/b': 16,\n",
       " '/v': 23,\n",
       " '/g': 12,\n",
       " '/d': 14,\n",
       " '/e': 2,\n",
       " '/zh': 3,\n",
       " '/z': 16,\n",
       " '/i': 10,\n",
       " '/y': 1,\n",
       " '/k': 20,\n",
       " '/l': 8,\n",
       " '/m': 16,\n",
       " '/n': 21,\n",
       " '/o': 25,\n",
       " '/p': 59,\n",
       " '/r': 18,\n",
       " '/s': 30,\n",
       " '/t': 13,\n",
       " '/u': 10,\n",
       " '/f': 7,\n",
       " '/h': 5,\n",
       " '/ts': 3,\n",
       " '/ch': 4,\n",
       " '/sh': 6,\n",
       " '/sch': 1,\n",
       " '/11': 1,\n",
       " '/y1': 1,\n",
       " '/22': 1,\n",
       " '/e1': 6,\n",
       " '/yu': 1,\n",
       " '/ya': 2}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letter_pages_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=['word', 'synonyms', 'synonyms_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "plgF8t_mPgRU"
   },
   "outputs": [],
   "source": [
    "def get_synonyms_list(driver, url, ref_to_synonyms):\n",
    "    driver.get(url+ref_to_synonyms)\n",
    "    bsoup = BeautifulSoup(dr.page_source, 'lxml')\n",
    "    syn_list = []\n",
    "    syn_table =  bsoup.find(class_='synonyms-table')\n",
    "    if syn_table is not None:\n",
    "        all_synonyms = syn_table.find('tbody').find_all('tr')\n",
    "        for syn_item in all_synonyms:\n",
    "            synonym = syn_item.find('a')\n",
    "            if synonym is None:\n",
    "                synonym = syn_item.find_all('td')[1].find('span').text\n",
    "            else:\n",
    "                synonym = synonym.text\n",
    "            syn_list.append(synonym)\n",
    "    return syn_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/z'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letter_refs[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "oCORydedbx9r",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [42:43<00:00,  2.56s/it]\n",
      "  6%|▋         | 1/16 [42:44<10:41:08, 2564.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page 1 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [41:59<00:00,  2.52s/it]\n",
      " 12%|█▎        | 2/16 [1:24:44<9:52:17, 2538.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page 2 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [41:47<00:00,  2.51s/it]\n",
      " 19%|█▉        | 3/16 [2:06:32<9:06:56, 2524.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page 3 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [40:57<00:00,  2.46s/it]\n",
      " 25%|██▌       | 4/16 [2:47:30<8:19:37, 2498.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page 4 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [41:28<00:00,  2.49s/it]\n",
      " 31%|███▏      | 5/16 [3:28:59<7:37:24, 2494.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page 5 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [42:22<00:00,  2.54s/it]\n",
      " 38%|███▊      | 6/16 [4:11:22<6:58:34, 2511.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page 6 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [45:23<00:00,  2.72s/it]\n",
      " 44%|████▍     | 7/16 [4:56:46<6:27:07, 2580.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page 7 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [46:54<00:00,  2.81s/it]\n",
      " 50%|█████     | 8/16 [5:43:41<5:54:03, 2655.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page 8 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [46:20<00:00,  2.78s/it]\n",
      " 56%|█████▋    | 9/16 [6:30:02<5:14:21, 2694.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page 9 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [48:36<00:00,  2.92s/it]\n",
      " 62%|██████▎   | 10/16 [7:18:39<4:36:19, 2763.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page 10 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [47:41<00:00,  2.86s/it]\n",
      " 69%|██████▉   | 11/16 [8:06:21<3:52:46, 2793.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page 11 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [50:19<00:00,  3.02s/it]\n",
      " 75%|███████▌  | 12/16 [8:56:41<3:10:49, 2862.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page 12 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [47:22<00:00,  2.84s/it]\n",
      " 81%|████████▏ | 13/16 [9:44:04<2:22:49, 2856.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page 13 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [48:23<00:00,  2.90s/it]\n",
      " 88%|████████▊ | 14/16 [10:32:28<1:35:41, 2870.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page 14 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [45:23<00:00,  2.72s/it]\n",
      " 94%|█████████▍| 15/16 [11:17:52<47:06, 2826.67s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page 15 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 651/651 [28:30<00:00,  2.63s/it]\n",
      "100%|██████████| 16/16 [11:46:23<00:00, 2648.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page 16 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ref = letter_refs[7]\n",
    "for page in tqdm(range(1, letter_pages_count[ref]+1)):\n",
    "    letter_page_df_list_format = []\n",
    "    if page == 1:\n",
    "        ref1 = ref\n",
    "    else:\n",
    "        ref1 = ref + '/' + str(page)\n",
    "    dr.get(url+ref1)\n",
    "    soup1 = BeautifulSoup(dr.page_source, 'lxml')\n",
    "    word_list = soup1.find('div', class_='wordlist-column synonyms').find_all('a')\n",
    "    for word in tqdm(word_list):\n",
    "        the_word = word.contents[0]\n",
    "        ref_to_synonyms = word.get('href')\n",
    "        synonyms_list = get_synonyms_list(dr, url, ref_to_synonyms)\n",
    "        synonyms_count = len(synonyms_list)\n",
    "        letter_page_df_list_format.append([the_word, synonyms_list, synonyms_count])\n",
    "    print(f'page {page} done')\n",
    "    letter_df = pd.DataFrame(data=letter_page_df_list_format, columns=columns)\n",
    "    letter_df.to_json(f'D:\\AtomicHackData\\synonyms_{ref[1:]}_{page}.json', force_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
